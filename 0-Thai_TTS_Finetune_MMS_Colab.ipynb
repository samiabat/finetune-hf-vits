{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiabat/thai-colab/blob/copilot%2Ffix-8cbcd932-40d8-4df6-b956-9da8ebff2d14/almostworking-Thai_TTS_Finetune_MMS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d66007",
      "metadata": {
        "id": "29d66007"
      },
      "source": [
        "\n",
        "# üáπüá≠ Thai TTS Fine-tuning (MMS-TTS ‚Üí VITS) ‚Äî Google Colab Notebook\n",
        "\n",
        "This notebook fine-tunes **Meta MMS-TTS (Thai)** on your own dataset using the **`finetune-hf-vits`** recipe.\n",
        "It assumes you already have:\n",
        "- a folder of WAV files (ideally mono 16 kHz), and\n",
        "- a transcript file (CSV/TSV) or a way to map each audio to its Thai text.\n",
        "\n",
        "> **Note:** MMS-TTS is released under **CC-BY-NC 4.0**. If you need commercial use, consider training from scratch or a different base.\n",
        "\n",
        "## ‚úÖ Recent Fixes\n",
        "- ‚úÖ Fixed configuration format to match `run_vits_finetuning.py` expectations\n",
        "- ‚úÖ Fixed dataset column naming: corrected `file_name` ‚Üí `path` to match training script expectations\n",
        "- ‚úÖ Resolved `ValueError: You are trying to load a dataset that was saved using save_to_disk` issue by switching to CSV format\n",
        "- ‚úÖ Fixed `Some keys are not used by the HfArgumentParser` error\n",
        "- ‚úÖ **NEW**: Fixed audio loading issues by implementing disk-based audio loading\n",
        "- ‚úÖ **NEW**: Updated to use correct repository: `samiabat/finetune-hf-vits` instead of `ylacombe/finetune-hf-vits`\n",
        "\n",
        "## üîß How Audio Loading Works Now\n",
        "\n",
        "The training script now automatically detects CSV files and loads audio from disk instead of using HuggingFace datasets:\n",
        "- **CSV Format**: Your dataset should be a CSV with `path` and `text` columns\n",
        "- **Audio Loading**: Audio files are loaded directly from disk using librosa\n",
        "- **Robust**: Handles various audio formats and sampling rates automatically\n",
        "- **Memory Efficient**: Audio is loaded on-demand during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6c895338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c895338",
        "outputId": "929271e8-6c92-4d57-a998-68ec08c5719e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch, sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('‚ö†Ô∏è No GPU detected. In Colab: Runtime ‚Üí Change runtime type ‚Üí T4/V100/A100 GPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a8d19c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a8d19c3",
        "outputId": "a1ff6673-e94f-491b-e95f-cc1545445600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content/finetune-hf-vits\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install --upgrade pip\n",
        "# Core\n",
        "!pip -q install torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers accelerate datasets soundfile librosa pythainlp pandas jiwer\n",
        "# Recipe\n",
        "!git clone -q https://github.com/samiabat/finetune-hf-vits.git\n",
        "%cd finetune-hf-vits\n",
        "!pip -q install -r requirements.txt\n",
        "%cd -\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79c6dde4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c6dde4",
        "outputId": "9d754ddb-f73a-4e2e-d578-641da824622a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('‚úÖ Google Drive mounted')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4357eb3e",
      "metadata": {
        "id": "4357eb3e"
      },
      "source": [
        "\n",
        "## Configure your paths\n",
        "\n",
        "- `DATA_ROOT`: the directory containing your WAV files (recursively scanned).\n",
        "- Either provide `TRANSCRIPT_CSV` (with `path,text`) **or** let the notebook build one from a folder structure+text file.\n",
        "- Output processed dataset and configs will be written under `WORK_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8be8e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8be8e93",
        "outputId": "c65d1d21-d8b8-4684-9428-88fe30568b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT  = /content/drive/MyDrive/cloned-thai-dataset/audio-data\n",
            "CSV        = /content/drive/MyDrive/cloned-thai-dataset/metadata.csv\n",
            "WORK_DIR   = /content/drive/MyDrive/thai_tts/work\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# ‚úÖ EDIT THESE\n",
        "DATA_ROOT = Path('/content/drive/MyDrive/cloned-thai-dataset/audio-data')   # folder with your wavs\n",
        "TRANSCRIPT_CSV = Path('/content/drive/MyDrive/cloned-thai-dataset/metadata.csv')  # CSV with columns: path,text (path absolute or relative to DATA_ROOT). Leave None if you don't have it.\n",
        "WORK_DIR = Path('/content/drive/MyDrive/thai_tts/work')    # where to write normalized data, configs, checkpoints\n",
        "\n",
        "# MMS language code for Thai\n",
        "LANG_CODE = 'tha'\n",
        "\n",
        "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print('DATA_ROOT  =', DATA_ROOT)\n",
        "print('CSV        =', TRANSCRIPT_CSV)\n",
        "print('WORK_DIR   =', WORK_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36210b6b",
      "metadata": {
        "id": "36210b6b"
      },
      "source": [
        "\n",
        "### (Optional) Create `metadata.csv` if you don't already have one\n",
        "\n",
        "If you **don't** have a CSV, this cell creates a simple CSV by scanning for `.wav` files and reading a paired `.txt` with the same basename for text (e.g., `utt001.wav` + `utt001.txt`).  \n",
        "Adjust as needed for your layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c0cdae8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cdae8d",
        "outputId": "33a188c2-2bce-44db-a010-7fa48dec8bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing metadata CSV: /content/drive/MyDrive/cloned-thai-dataset/metadata.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def build_metadata_from_sidecar_txt(data_root: Path, out_csv: Path):\n",
        "    rows = []\n",
        "    for wav in data_root.rglob('*.wav'):\n",
        "        txt = wav.with_suffix('.txt')\n",
        "        if txt.exists():\n",
        "            text = txt.read_text(encoding='utf-8').strip()\n",
        "            rows.append({'path': str(wav.resolve()), 'text': text})\n",
        "    if not rows:\n",
        "        raise ValueError('No (wav, txt) pairs found. Provide TRANSCRIPT_CSV instead.')\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    return out_csv\n",
        "\n",
        "if str(TRANSCRIPT_CSV).lower() == 'none':\n",
        "    TRANSCRIPT_CSV = WORK_DIR / 'metadata.csv'\n",
        "    created = build_metadata_from_sidecar_txt(DATA_ROOT, TRANSCRIPT_CSV)\n",
        "    print('Created CSV at', created)\n",
        "else:\n",
        "    print('Using existing metadata CSV:', TRANSCRIPT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46971292",
      "metadata": {
        "id": "46971292"
      },
      "source": [
        "\n",
        "## Preprocess audio & normalize Thai text\n",
        "\n",
        "- Resample/convert to **mono 16 kHz WAV**\n",
        "- Light Thai normalization (using PyThaiNLP)\n",
        "- Filter clips (1‚Äì12 s recommended)\n",
        "- Produce a cleaned `metadata_clean.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "56d85516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "56d85516",
        "outputId": "35d73b93-55a4-4d3a-efbc-852b7564f63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote cleaned CSV: /content/drive/MyDrive/thai_tts/work/metadata_clean.csv\n",
            "Total usable clips: 521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           file_name  \\\n",
              "0  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "1  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "2  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "3  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "4  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "\n",
              "                                                text  \n",
              "0  ‡πÄ‡∏ä‡πâ‡∏≤   ‡∏ï‡πâ‡∏≠‡∏á   ‡πÄ‡∏´‡πá‡∏ô   ‡∏´‡∏ô‡πâ‡∏≤   ‡πÄ‡∏¢‡πá‡∏ô   ‡∏Å‡πá   ‡∏ï‡πâ‡∏≠‡∏á  ...  \n",
              "1                                        ‡∏î‡∏¥‡πâ‡∏ô   ‡∏î‡∏¥‡πâ‡∏ô  \n",
              "2                                              ‡∏Å‡∏≥‡∏à‡∏±‡∏î  \n",
              "3                                 ‡πÄ‡∏Ñ‡∏≤‡∏∞   ‡πÑ‡∏•‡πà   ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®  \n",
              "4                                      ‡πÄ‡∏ß‡πá‡∏ö   ‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d056565-6e0f-4e02-83db-2acc6f12cf04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>‡πÄ‡∏ä‡πâ‡∏≤   ‡∏ï‡πâ‡∏≠‡∏á   ‡πÄ‡∏´‡πá‡∏ô   ‡∏´‡∏ô‡πâ‡∏≤   ‡πÄ‡∏¢‡πá‡∏ô   ‡∏Å‡πá   ‡∏ï‡πâ‡∏≠‡∏á  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>‡∏î‡∏¥‡πâ‡∏ô   ‡∏î‡∏¥‡πâ‡∏ô</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>‡∏Å‡∏≥‡∏à‡∏±‡∏î</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>‡πÄ‡∏Ñ‡∏≤‡∏∞   ‡πÑ‡∏•‡πà   ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>‡πÄ‡∏ß‡πá‡∏ö   ‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d056565-6e0f-4e02-83db-2acc6f12cf04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d056565-6e0f-4e02-83db-2acc6f12cf04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d056565-6e0f-4e02-83db-2acc6f12cf04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-00f4115a-7f54-4867-903f-b85576a93f1f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00f4115a-7f54-4867-903f-b85576a93f1f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-00f4115a-7f54-4867-903f-b85576a93f1f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clean_df",
              "summary": "{\n  \"name\": \"clean_df\",\n  \"rows\": 521,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 521,\n        \"samples\": [\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S537_16k.wav\",\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S115_16k.wav\",\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S011_16k.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 521,\n        \"samples\": [\n          \"\\u0e2d\\u0e22\\u0e39\\u0e48   \\u0e04\\u0e38\\u0e49\\u0e21\\u0e04\\u0e48\\u0e32\",\n          \"\\u0e40\\u0e2a\\u0e49\\u0e19\\u0e40\\u0e25\\u0e37\\u0e2d\\u0e14\",\n          \"\\u0e2d\\u0e33\\u0e19\\u0e27\\u0e22\\u0e04\\u0e27\\u0e32\\u0e21\\u0e2a\\u0e30\\u0e14\\u0e27\\u0e01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "import librosa, soundfile as sf, pandas as pd, numpy as np, os\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "PROC_AUDIO_DIR = WORK_DIR / 'wavs_16k_mono'\n",
        "PROC_AUDIO_DIR.mkdir(exist_ok=True, parents=True)\n",
        "OUT_CSV = WORK_DIR / 'metadata_clean.csv'\n",
        "\n",
        "MIN_DUR = 1.0\n",
        "MAX_DUR = 12.0\n",
        "TARGET_SR = 16000\n",
        "\n",
        "def normalize_thai(text: str) -> str:\n",
        "    # Minimal normalization: collapse spaces; optional segmentation (helps prosody)\n",
        "    seg = word_tokenize(text.strip(), engine='newmm')\n",
        "    return ' '.join(seg)\n",
        "\n",
        "df = pd.read_csv(TRANSCRIPT_CSV)\n",
        "clean_rows = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    src = Path('/content/drive/MyDrive/cloned-thai-dataset/' + row['file_name'])\n",
        "    text = str(row['text'])\n",
        "    if not src.exists():\n",
        "        print('Skip (missing):', src)\n",
        "        continue\n",
        "    try:\n",
        "        wav, sr = librosa.load(src, sr=None, mono=True)\n",
        "        dur = len(wav)/sr\n",
        "        if dur < MIN_DUR or dur > MAX_DUR:\n",
        "            continue\n",
        "        if sr != TARGET_SR:\n",
        "            wav = librosa.resample(wav, orig_sr=sr, target_sr=TARGET_SR)\n",
        "            sr = TARGET_SR\n",
        "        # Write processed wav under WORK_DIR, mirroring file name\n",
        "        out_wav = PROC_AUDIO_DIR / f\"{src.stem}_16k.wav\"\n",
        "        sf.write(out_wav, wav, sr, subtype='PCM_16')\n",
        "        clean_rows.append({'path': str(out_wav), 'text': normalize_thai(text)})\n",
        "    except Exception as e:\n",
        "        print('Error:', src, e)\n",
        "\n",
        "clean_df = pd.DataFrame(clean_rows)\n",
        "clean_df.to_csv(OUT_CSV, index=False)\n",
        "print('Wrote cleaned CSV:', OUT_CSV)\n",
        "print('Total usable clips:', len(clean_df))\n",
        "clean_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eaf84b8",
      "metadata": {
        "id": "3eaf84b8"
      },
      "source": [
        "\n",
        "### Prepare dataset for training\n",
        "\n",
        "The `finetune-hf-vits` script can read from a HuggingFace dataset or from local CSV.  \n",
        "This step creates a cleaned CSV file that can be directly used by the training script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5085d64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "5bbe1dd822ef4f1e9b18e022d6004ac9",
            "ecaae89771af441bba58244a7ddfcb9d",
            "0b464c65ddb84c54bd42f164ccb0d0f4",
            "15df518a41d14af695a82859a6bc75d2",
            "f3a4f6e4a41d44a5abb98977f7b5afdd",
            "467a7fbcae96452c885391eb4fea0bbc",
            "c1496daec1ea4938958681b8d9c8b704",
            "84f8e4f99d6246f39f1c4722e8f83ea5",
            "37b964789a4e4cfe8f597c657cbe6654",
            "fcf593aa563b469ba537afb4f0a7754c",
            "7b03ee6107e84dda971f18493df63178"
          ]
        },
        "id": "b5085d64",
        "outputId": "78baf024-ba40-486b-eef9-1a0cf06467c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/521 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bbe1dd822ef4f1e9b18e022d6004ac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved HuggingFace dataset at: /content/drive/MyDrive/thai_tts/work/hf_dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV and create a cleaned version for training\n",
        "df = pd.read_csv(OUT_CSV)\n",
        "\n",
        "# Ensure column names match training script expectations\n",
        "# The training script expects 'path' and 'text' columns\n",
        "if 'file_name' in df.columns:\n",
        "    df = df.rename(columns={'file_name': 'path'})\n",
        "\n",
        "# Save the cleaned CSV that can be directly used by the training script\n",
        "cleaned_csv_path = WORK_DIR / 'metadata_clean.csv'\n",
        "df.to_csv(cleaned_csv_path, index=False)\n",
        "print(f'Saved cleaned CSV for training at: {cleaned_csv_path}')\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'Columns: {list(df.columns)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbeddc2",
      "metadata": {
        "id": "adbeddc2"
      },
      "source": [
        "\n",
        "## Prepare MMS-TTS (Thai) checkpoint for training\n",
        "\n",
        "This uses `convert_original_discriminator_checkpoint.py` from the recipe to create a trainable VITS-style checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db90c60a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db90c60a",
        "outputId": "391225f6-c461-4604-8868-5143848607e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits\n",
            "2025-09-19 05:38:01.243442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758260281.263555    2603 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758260281.269682    2603 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758260281.285422    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285450    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285454    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285467    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-19 05:38:01.290114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "full_models/tha/D_100000.pth: 100% 561M/561M [00:07<00:00, 74.4MB/s]\n",
            "config.json: 1.64kB [00:00, 8.49MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/config.json\n",
            "Model config VitsConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"architectures\": [\n",
            "    \"VitsModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"depth_separable_channels\": 2,\n",
            "  \"depth_separable_num_layers\": 3,\n",
            "  \"discriminator_kernel_size\": 5,\n",
            "  \"discriminator_period_channels\": [\n",
            "    1,\n",
            "    32,\n",
            "    128,\n",
            "    512,\n",
            "    1024\n",
            "  ],\n",
            "  \"discriminator_periods\": [\n",
            "    2,\n",
            "    3,\n",
            "    5,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"discriminator_scale_channels\": [\n",
            "    1,\n",
            "    16,\n",
            "    64,\n",
            "    256,\n",
            "    1024\n",
            "  ],\n",
            "  \"discriminator_stride\": 3,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"duration_predictor_dropout\": 0.5,\n",
            "  \"duration_predictor_filter_channels\": 256,\n",
            "  \"duration_predictor_flow_bins\": 10,\n",
            "  \"duration_predictor_kernel_size\": 3,\n",
            "  \"duration_predictor_num_flows\": 4,\n",
            "  \"duration_predictor_tail_bound\": 5.0,\n",
            "  \"ffn_dim\": 768,\n",
            "  \"ffn_kernel_size\": 3,\n",
            "  \"flow_size\": 192,\n",
            "  \"hidden_act\": \"relu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 192,\n",
            "  \"hop_length\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"leaky_relu_slope\": 0.1,\n",
            "  \"model_type\": \"vits\",\n",
            "  \"noise_scale\": 0.667,\n",
            "  \"noise_scale_duration\": 0.8,\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"num_speakers\": 1,\n",
            "  \"posterior_encoder_num_wavenet_layers\": 16,\n",
            "  \"prior_encoder_num_flows\": 4,\n",
            "  \"prior_encoder_num_wavenet_layers\": 4,\n",
            "  \"resblock_dilation_sizes\": [\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ]\n",
            "  ],\n",
            "  \"resblock_kernel_sizes\": [\n",
            "    3,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"sampling_rate\": 16000,\n",
            "  \"segment_size\": 8192,\n",
            "  \"speaker_embedding_size\": 0,\n",
            "  \"speaking_rate\": 1.0,\n",
            "  \"spectrogram_bins\": 513,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"upsample_initial_channel\": 512,\n",
            "  \"upsample_kernel_sizes\": [\n",
            "    16,\n",
            "    16,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"upsample_rates\": [\n",
            "    8,\n",
            "    8,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"use_bias\": true,\n",
            "  \"use_stochastic_duration_prediction\": true,\n",
            "  \"vocab_size\": 71,\n",
            "  \"wavenet_dilation_rate\": 1,\n",
            "  \"wavenet_dropout\": 0.0,\n",
            "  \"wavenet_kernel_size\": 5,\n",
            "  \"window_size\": 4\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/config.json\n",
            "Model config VitsConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"architectures\": [\n",
            "    \"VitsModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"depth_separable_channels\": 2,\n",
            "  \"depth_separable_num_layers\": 3,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"duration_predictor_dropout\": 0.5,\n",
            "  \"duration_predictor_filter_channels\": 256,\n",
            "  \"duration_predictor_flow_bins\": 10,\n",
            "  \"duration_predictor_kernel_size\": 3,\n",
            "  \"duration_predictor_num_flows\": 4,\n",
            "  \"duration_predictor_tail_bound\": 5.0,\n",
            "  \"ffn_dim\": 768,\n",
            "  \"ffn_kernel_size\": 3,\n",
            "  \"flow_size\": 192,\n",
            "  \"hidden_act\": \"relu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 192,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"leaky_relu_slope\": 0.1,\n",
            "  \"model_type\": \"vits\",\n",
            "  \"noise_scale\": 0.667,\n",
            "  \"noise_scale_duration\": 0.8,\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"num_speakers\": 1,\n",
            "  \"posterior_encoder_num_wavenet_layers\": 16,\n",
            "  \"prior_encoder_num_flows\": 4,\n",
            "  \"prior_encoder_num_wavenet_layers\": 4,\n",
            "  \"resblock_dilation_sizes\": [\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ]\n",
            "  ],\n",
            "  \"resblock_kernel_sizes\": [\n",
            "    3,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"sampling_rate\": 16000,\n",
            "  \"speaker_embedding_size\": 0,\n",
            "  \"speaking_rate\": 1.0,\n",
            "  \"spectrogram_bins\": 513,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"upsample_initial_channel\": 512,\n",
            "  \"upsample_kernel_sizes\": [\n",
            "    16,\n",
            "    16,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"upsample_rates\": [\n",
            "    8,\n",
            "    8,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"use_bias\": true,\n",
            "  \"use_stochastic_duration_prediction\": true,\n",
            "  \"vocab_size\": 71,\n",
            "  \"wavenet_dilation_rate\": 1,\n",
            "  \"wavenet_dropout\": 0.0,\n",
            "  \"wavenet_kernel_size\": 5,\n",
            "  \"window_size\": 4\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 145M/145M [00:01<00:00, 96.6MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/model.safetensors\n",
            "All model checkpoint weights were used when initializing VitsModel.\n",
            "\n",
            "All the weights of VitsModel were initialized from the model checkpoint at facebook/mms-tts-tha.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VitsModel for predictions without further training.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "model loaded: 46.7M params\n",
            "tokenizer_config.json: 100% 289/289 [00:00<00:00, 1.98MB/s]\n",
            "vocab.json: 100% 902/902 [00:00<00:00, 6.77MB/s]\n",
            "special_tokens_map.json: 100% 49.0/49.0 [00:00<00:00, 336kB/s]\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/vocab.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file chat_template.jinja from cache at None\n",
            "Configuration saved in /content/mms-tha-train/config.json\n",
            "Model weights saved in /content/mms-tha-train/model.safetensors\n",
            "tokenizer config file saved in /content/mms-tha-train/tokenizer_config.json\n",
            "Special tokens file saved in /content/mms-tha-train/special_tokens_map.json\n",
            "added tokens file saved in /content/mms-tha-train/added_tokens.json\n",
            "Feature extractor saved in /content/mms-tha-train/preprocessor_config.json\n",
            "total 317M\n",
            "drwxr-xr-x 2 root root 4.0K Sep 19 05:38 .\n",
            "drwxr-xr-x 1 root root 4.0K Sep 19 05:38 ..\n",
            "-rw-r--r-- 1 root root   18 Sep 19 05:38 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.0K Sep 19 05:38 config.json\n",
            "-rw-r--r-- 1 root root 317M Sep 19 05:38 model.safetensors\n",
            "-rw-r--r-- 1 root root  254 Sep 19 05:38 preprocessor_config.json\n",
            "-rw-r--r-- 1 root root   49 Sep 19 05:38 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  704 Sep 19 05:38 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  902 Sep 19 05:38 vocab.json\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/finetune-hf-vits\n",
        "\n",
        "!python convert_original_discriminator_checkpoint.py   --language_code {LANG_CODE}   --pytorch_dump_folder_path /content/mms-tha-train\n",
        "\n",
        "# (Optional) list files\n",
        "!ls -lah /content/mms-tha-train\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c82a17",
      "metadata": {
        "id": "74c82a17"
      },
      "source": [
        "\n",
        "## Training config\n",
        "\n",
        "Adjust batch size, learning rate, max steps/epochs to your GPU and dataset size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0f0ccca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0f0ccca",
        "outputId": "06cca9b9-0bbc-468c-d4e4-4548ee235f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_name_or_path\": \"/content/mms-tha-train\",\n",
            "  \"dataset_name\": \"/content/drive/MyDrive/thai_tts/work/hf_dataset\",\n",
            "  \"audio_column_name\": \"path\",\n",
            "  \"text_column_name\": \"text\",\n",
            "  \"train_split_name\": \"train\",\n",
            "  \"eval_split_name\": \"train\",\n",
            "  \"output_dir\": \"/content/tts-tha-checkpoints\",\n",
            "  \"do_train\": true,\n",
            "  \"per_device_train_batch_size\": 8,\n",
            "  \"gradient_accumulation_steps\": 2,\n",
            "  \"learning_rate\": 0.0001,\n",
            "  \"max_steps\": 20000,\n",
            "  \"logging_steps\": 50,\n",
            "  \"save_steps\": 1000,\n",
            "  \"eval_steps\": 1000,\n",
            "  \"warmup_steps\": 500,\n",
            "  \"fp16\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json, os\n",
        "cfg = {\n",
        "  # Model arguments\n",
        "  \"model_name_or_path\": \"/content/mms-tha-train\",\n",
        "  \n",
        "  # Data arguments - using CSV instead of save_to_disk format\n",
        "  \"dataset_name\": str(WORK_DIR / 'metadata_clean.csv'),\n",
        "  \"audio_column_name\": \"path\",\n",
        "  \"text_column_name\": \"text\",\n",
        "  \"train_split_name\": \"train\",\n",
        "  \"eval_split_name\": \"train\",\n",
        "  \n",
        "  # Training arguments\n",
        "  \"output_dir\": \"/content/tts-tha-checkpoints\",\n",
        "  \"do_train\": True,\n",
        "  \"per_device_train_batch_size\": 8,\n",
        "  \"gradient_accumulation_steps\": 2,\n",
        "  \"learning_rate\": 0.0001,\n",
        "  \"max_steps\": 20000,\n",
        "  \"logging_steps\": 50,\n",
        "  \"save_steps\": 1000,\n",
        "  \"eval_steps\": 1000,\n",
        "  \"warmup_steps\": 500,\n",
        "  \"fp16\": True\n",
        "}\n",
        "os.makedirs('/content/config', exist_ok=True)\n",
        "with open('/content/config/train_thai.json', 'w') as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "print(open('/content/config/train_thai.json').read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b810967e",
      "metadata": {
        "id": "b810967e"
      },
      "source": [
        "\n",
        "## Launch training\n",
        "\n",
        "This will start fine-tuning the Thai MMS-TTS model on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "EIOqSH8Tfzvf",
        "outputId": "c8d30cde-7283-49d4-fd41-4a0e8ac758f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "EIOqSH8Tfzvf",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/finetune-hf-vits/monotonic_align"
      ],
      "metadata": {
        "id": "1idEwjr9f995",
        "outputId": "b77b0e62-2dc0-4e95-e42a-deeb558498d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1idEwjr9f995",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits/monotonic_align\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "6MpWFW16gAyB"
      },
      "id": "6MpWFW16gAyB",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dbf7b6c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbf7b6c4",
        "outputId": "44eb9521-776d-406a-96e3-d208f7c6ba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-09-19 05:45:06.625833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758260706.661256    4490 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758260706.671421    4490 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758260706.698965    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699009    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699017    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699022    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-19 05:45:06.706729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetune-hf-vits/run_vits_finetuning.py\", line 1494, in <module>\n",
            "    main()\n",
            "  File \"/content/finetune-hf-vits/run_vits_finetuning.py\", line 534, in main\n",
            "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/hf_argparser.py\", line 420, in parse_json_file\n",
            "    outputs = self.parse_dict(data, allow_extra_keys=allow_extra_keys)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/hf_argparser.py\", line 396, in parse_dict\n",
            "    raise ValueError(f\"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}\")\n",
            "ValueError: Some keys are not used by the HfArgumentParser: ['dataset', 'train']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1235, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 823, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'run_vits_finetuning.py', '/content/config/train_thai.json']' returned non-zero exit status 1.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/finetune-hf-vits\n",
        "!accelerate launch run_vits_finetuning.py /content/config/train_thai.json\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697e8777",
      "metadata": {
        "id": "697e8777"
      },
      "source": [
        "\n",
        "## Quick inference test\n",
        "\n",
        "Generate a sample audio using the fine-tuned checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5006385",
      "metadata": {
        "id": "f5006385"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "import soundfile as sf\n",
        "\n",
        "ckpt_dir = \"/content/tts-tha-checkpoints\"\n",
        "tts = pipeline(\"text-to-speech\", model=ckpt_dir, device=0 if torch.cuda.is_available() else -1)\n",
        "sample_text = \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\"\n",
        "out = tts(sample_text)\n",
        "sf.write('/content/sample_thai_tts.wav', out[\"audio\"], out[\"sampling_rate\"], subtype='PCM_16')\n",
        "print('Saved:', '/content/sample_thai_tts.wav')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b2fe3a",
      "metadata": {
        "id": "06b2fe3a"
      },
      "source": [
        "\n",
        "## Save to Drive (and optionally push to Hugging Face Hub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25ec607",
      "metadata": {
        "id": "f25ec607"
      },
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "drive_ckpt = str(WORK_DIR / 'checkpoints_mms_thai')\n",
        "shutil.copytree('/content/tts-tha-checkpoints', drive_ckpt, dirs_exist_ok=True)\n",
        "print('Copied checkpoints to:', drive_ckpt)\n",
        "\n",
        "# Optional: push to Hub (uncomment and set your repo)\n",
        "# !pip -q install huggingface_hub\n",
        "# from huggingface_hub import HfApi, create_repo\n",
        "# repo_id = \"yourname/mms-thai-finetuned\"\n",
        "# create_repo(repo_id, private=True, exist_ok=True)\n",
        "# !huggingface-cli login\n",
        "# !git lfs install\n",
        "# !git init /content/tts-tha-checkpoints\n",
        "# %cd /content/tts-tha-checkpoints\n",
        "# !git remote add origin https://huggingface.co/yourname/mms-thai-finetuned\n",
        "# !git add . && git commit -m \"Add Thai TTS fine-tuned\" && git push -u origin main\n",
        "# %cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e07531b",
      "metadata": {
        "id": "6e07531b"
      },
      "source": [
        "\n",
        "### Notes & Tips\n",
        "- Recommended 1‚Äì3 hours clean audio for decent cloning; 5‚Äì10+ hours for robust style control.\n",
        "- Keep clips 1‚Äì12 seconds; remove background noise as much as possible.\n",
        "- If you get OOM (out-of-memory), reduce `per_device_train_batch_size` or increase `gradient_accumulation_steps`.\n",
        "- If training is slow, reduce `max_steps` initially to validate the pipeline, then scale up.\n",
        "\n",
        "### üéØ Current Approach: CSV-based Audio Loading\n",
        "This notebook now uses CSV-based audio loading by default, which is more robust:\n",
        "1. ‚úÖ Audio files are loaded directly from disk (not stored in HuggingFace datasets)\n",
        "2. ‚úÖ Better memory management and compatibility\n",
        "3. ‚úÖ Automatic audio format conversion and resampling\n",
        "4. ‚úÖ The training script automatically detects `.csv` files and uses the new loading method\n",
        "\n",
        "**Your CSV should have these columns:**\n",
        "- `path`: Full path to the audio file\n",
        "- `text`: Corresponding text transcription\n",
        "\n",
        "**Example CSV:**\n",
        "```\n",
        "path,text\n",
        "/path/to/audio1.wav,‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö\n",
        "/path/to/audio2.wav,‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5bbe1dd822ef4f1e9b18e022d6004ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecaae89771af441bba58244a7ddfcb9d",
              "IPY_MODEL_0b464c65ddb84c54bd42f164ccb0d0f4",
              "IPY_MODEL_15df518a41d14af695a82859a6bc75d2"
            ],
            "layout": "IPY_MODEL_f3a4f6e4a41d44a5abb98977f7b5afdd"
          }
        },
        "ecaae89771af441bba58244a7ddfcb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467a7fbcae96452c885391eb4fea0bbc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c1496daec1ea4938958681b8d9c8b704",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "0b464c65ddb84c54bd42f164ccb0d0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f8e4f99d6246f39f1c4722e8f83ea5",
            "max": 521,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37b964789a4e4cfe8f597c657cbe6654",
            "value": 521
          }
        },
        "15df518a41d14af695a82859a6bc75d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf593aa563b469ba537afb4f0a7754c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7b03ee6107e84dda971f18493df63178",
            "value": "‚Äá521/521‚Äá[00:01&lt;00:00,‚Äá381.93‚Äáexamples/s]"
          }
        },
        "f3a4f6e4a41d44a5abb98977f7b5afdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467a7fbcae96452c885391eb4fea0bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1496daec1ea4938958681b8d9c8b704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84f8e4f99d6246f39f1c4722e8f83ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b964789a4e4cfe8f597c657cbe6654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcf593aa563b469ba537afb4f0a7754c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b03ee6107e84dda971f18493df63178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}