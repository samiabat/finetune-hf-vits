{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiabat/thai-colab/blob/copilot%2Ffix-8cbcd932-40d8-4df6-b956-9da8ebff2d14/almostworking-Thai_TTS_Finetune_MMS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d66007",
      "metadata": {
        "id": "29d66007"
      },
      "source": [
        "\n",
        "# 🇹🇭 Thai TTS Fine-tuning (MMS-TTS → VITS) — Google Colab Notebook\n",
        "\n",
        "This notebook fine-tunes **Meta MMS-TTS (Thai)** on your own dataset using the **`finetune-hf-vits`** recipe.\n",
        "It assumes you already have:\n",
        "- a folder of WAV files (ideally mono 16 kHz), and\n",
        "- a transcript file (CSV/TSV) or a way to map each audio to its Thai text.\n",
        "\n",
        "> **Note:** MMS-TTS is released under **CC-BY-NC 4.0**. If you need commercial use, consider training from scratch or a different base.\n",
        "\n",
        "## ✅ Recent Fixes\n",
        "- ✅ Fixed configuration format to match `run_vits_finetuning.py` expectations\n",
        "- ✅ Fixed dataset column naming: corrected `file_name` → `path` to match training script expectations\n",
        "- ✅ Resolved `ValueError: You are trying to load a dataset that was saved using save_to_disk` issue by switching to CSV format\n",
        "- ✅ Fixed `Some keys are not used by the HfArgumentParser` error\n",
        "- ✅ **NEW**: Fixed audio loading issues by implementing disk-based audio loading\n",
        "- ✅ **NEW**: Updated to use correct repository: `samiabat/finetune-hf-vits` instead of `ylacombe/finetune-hf-vits`\n",
        "\n",
        "## 🔧 How Audio Loading Works Now\n",
        "\n",
        "The training script now automatically detects CSV files and loads audio from disk instead of using HuggingFace datasets:\n",
        "- **CSV Format**: Your dataset should be a CSV with `path` and `text` columns\n",
        "- **Audio Loading**: Audio files are loaded directly from disk using librosa\n",
        "- **Robust**: Handles various audio formats and sampling rates automatically\n",
        "- **Memory Efficient**: Audio is loaded on-demand during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6c895338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c895338",
        "outputId": "929271e8-6c92-4d57-a998-68ec08c5719e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch, sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('⚠️ No GPU detected. In Colab: Runtime → Change runtime type → T4/V100/A100 GPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a8d19c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a8d19c3",
        "outputId": "a1ff6673-e94f-491b-e95f-cc1545445600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content/finetune-hf-vits\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install --upgrade pip\n",
        "# Core\n",
        "!pip -q install torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers accelerate datasets soundfile librosa pythainlp pandas jiwer\n",
        "# Recipe\n",
        "!git clone -q https://github.com/samiabat/finetune-hf-vits.git\n",
        "%cd finetune-hf-vits\n",
        "!pip -q install -r requirements.txt\n",
        "%cd -\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79c6dde4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c6dde4",
        "outputId": "9d754ddb-f73a-4e2e-d578-641da824622a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('✅ Google Drive mounted')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4357eb3e",
      "metadata": {
        "id": "4357eb3e"
      },
      "source": [
        "\n",
        "## Configure your paths\n",
        "\n",
        "- `DATA_ROOT`: the directory containing your WAV files (recursively scanned).\n",
        "- Either provide `TRANSCRIPT_CSV` (with `path,text`) **or** let the notebook build one from a folder structure+text file.\n",
        "- Output processed dataset and configs will be written under `WORK_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8be8e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8be8e93",
        "outputId": "c65d1d21-d8b8-4684-9428-88fe30568b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT  = /content/drive/MyDrive/cloned-thai-dataset/audio-data\n",
            "CSV        = /content/drive/MyDrive/cloned-thai-dataset/metadata.csv\n",
            "WORK_DIR   = /content/drive/MyDrive/thai_tts/work\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# ✅ EDIT THESE\n",
        "DATA_ROOT = Path('/content/drive/MyDrive/cloned-thai-dataset/audio-data')   # folder with your wavs\n",
        "TRANSCRIPT_CSV = Path('/content/drive/MyDrive/cloned-thai-dataset/metadata.csv')  # CSV with columns: path,text (path absolute or relative to DATA_ROOT). Leave None if you don't have it.\n",
        "WORK_DIR = Path('/content/drive/MyDrive/thai_tts/work')    # where to write normalized data, configs, checkpoints\n",
        "\n",
        "# MMS language code for Thai\n",
        "LANG_CODE = 'tha'\n",
        "\n",
        "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print('DATA_ROOT  =', DATA_ROOT)\n",
        "print('CSV        =', TRANSCRIPT_CSV)\n",
        "print('WORK_DIR   =', WORK_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36210b6b",
      "metadata": {
        "id": "36210b6b"
      },
      "source": [
        "\n",
        "### (Optional) Create `metadata.csv` if you don't already have one\n",
        "\n",
        "If you **don't** have a CSV, this cell creates a simple CSV by scanning for `.wav` files and reading a paired `.txt` with the same basename for text (e.g., `utt001.wav` + `utt001.txt`).  \n",
        "Adjust as needed for your layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c0cdae8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cdae8d",
        "outputId": "33a188c2-2bce-44db-a010-7fa48dec8bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing metadata CSV: /content/drive/MyDrive/cloned-thai-dataset/metadata.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def build_metadata_from_sidecar_txt(data_root: Path, out_csv: Path):\n",
        "    rows = []\n",
        "    for wav in data_root.rglob('*.wav'):\n",
        "        txt = wav.with_suffix('.txt')\n",
        "        if txt.exists():\n",
        "            text = txt.read_text(encoding='utf-8').strip()\n",
        "            rows.append({'path': str(wav.resolve()), 'text': text})\n",
        "    if not rows:\n",
        "        raise ValueError('No (wav, txt) pairs found. Provide TRANSCRIPT_CSV instead.')\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    return out_csv\n",
        "\n",
        "if str(TRANSCRIPT_CSV).lower() == 'none':\n",
        "    TRANSCRIPT_CSV = WORK_DIR / 'metadata.csv'\n",
        "    created = build_metadata_from_sidecar_txt(DATA_ROOT, TRANSCRIPT_CSV)\n",
        "    print('Created CSV at', created)\n",
        "else:\n",
        "    print('Using existing metadata CSV:', TRANSCRIPT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46971292",
      "metadata": {
        "id": "46971292"
      },
      "source": [
        "\n",
        "## Preprocess audio & normalize Thai text\n",
        "\n",
        "- Resample/convert to **mono 16 kHz WAV**\n",
        "- Light Thai normalization (using PyThaiNLP)\n",
        "- Filter clips (1–12 s recommended)\n",
        "- Produce a cleaned `metadata_clean.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "56d85516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "56d85516",
        "outputId": "35d73b93-55a4-4d3a-efbc-852b7564f63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote cleaned CSV: /content/drive/MyDrive/thai_tts/work/metadata_clean.csv\n",
            "Total usable clips: 521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           file_name  \\\n",
              "0  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "1  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "2  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "3  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "4  /content/drive/MyDrive/thai_tts/work/wavs_16k_...   \n",
              "\n",
              "                                                text  \n",
              "0  เช้า   ต้อง   เห็น   หน้า   เย็น   ก็   ต้อง  ...  \n",
              "1                                        ดิ้น   ดิ้น  \n",
              "2                                              กำจัด  \n",
              "3                                 เคาะ   ไล่   อากาศ  \n",
              "4                                      เว็บ   ดีไซน์  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d056565-6e0f-4e02-83db-2acc6f12cf04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>เช้า   ต้อง   เห็น   หน้า   เย็น   ก็   ต้อง  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>ดิ้น   ดิ้น</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>กำจัด</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>เคาะ   ไล่   อากาศ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/thai_tts/work/wavs_16k_...</td>\n",
              "      <td>เว็บ   ดีไซน์</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d056565-6e0f-4e02-83db-2acc6f12cf04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d056565-6e0f-4e02-83db-2acc6f12cf04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d056565-6e0f-4e02-83db-2acc6f12cf04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-00f4115a-7f54-4867-903f-b85576a93f1f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00f4115a-7f54-4867-903f-b85576a93f1f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-00f4115a-7f54-4867-903f-b85576a93f1f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clean_df",
              "summary": "{\n  \"name\": \"clean_df\",\n  \"rows\": 521,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 521,\n        \"samples\": [\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S537_16k.wav\",\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S115_16k.wav\",\n          \"/content/drive/MyDrive/thai_tts/work/wavs_16k_mono/G0501_S011_16k.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 521,\n        \"samples\": [\n          \"\\u0e2d\\u0e22\\u0e39\\u0e48   \\u0e04\\u0e38\\u0e49\\u0e21\\u0e04\\u0e48\\u0e32\",\n          \"\\u0e40\\u0e2a\\u0e49\\u0e19\\u0e40\\u0e25\\u0e37\\u0e2d\\u0e14\",\n          \"\\u0e2d\\u0e33\\u0e19\\u0e27\\u0e22\\u0e04\\u0e27\\u0e32\\u0e21\\u0e2a\\u0e30\\u0e14\\u0e27\\u0e01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "import librosa, soundfile as sf, pandas as pd, numpy as np, os\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "PROC_AUDIO_DIR = WORK_DIR / 'wavs_16k_mono'\n",
        "PROC_AUDIO_DIR.mkdir(exist_ok=True, parents=True)\n",
        "OUT_CSV = WORK_DIR / 'metadata_clean.csv'\n",
        "\n",
        "MIN_DUR = 1.0\n",
        "MAX_DUR = 12.0\n",
        "TARGET_SR = 16000\n",
        "\n",
        "def normalize_thai(text: str) -> str:\n",
        "    # Minimal normalization: collapse spaces; optional segmentation (helps prosody)\n",
        "    seg = word_tokenize(text.strip(), engine='newmm')\n",
        "    return ' '.join(seg)\n",
        "\n",
        "df = pd.read_csv(TRANSCRIPT_CSV)\n",
        "clean_rows = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    src = Path('/content/drive/MyDrive/cloned-thai-dataset/' + row['file_name'])\n",
        "    text = str(row['text'])\n",
        "    if not src.exists():\n",
        "        print('Skip (missing):', src)\n",
        "        continue\n",
        "    try:\n",
        "        wav, sr = librosa.load(src, sr=None, mono=True)\n",
        "        dur = len(wav)/sr\n",
        "        if dur < MIN_DUR or dur > MAX_DUR:\n",
        "            continue\n",
        "        if sr != TARGET_SR:\n",
        "            wav = librosa.resample(wav, orig_sr=sr, target_sr=TARGET_SR)\n",
        "            sr = TARGET_SR\n",
        "        # Write processed wav under WORK_DIR, mirroring file name\n",
        "        out_wav = PROC_AUDIO_DIR / f\"{src.stem}_16k.wav\"\n",
        "        sf.write(out_wav, wav, sr, subtype='PCM_16')\n",
        "        clean_rows.append({'path': str(out_wav), 'text': normalize_thai(text)})\n",
        "    except Exception as e:\n",
        "        print('Error:', src, e)\n",
        "\n",
        "clean_df = pd.DataFrame(clean_rows)\n",
        "clean_df.to_csv(OUT_CSV, index=False)\n",
        "print('Wrote cleaned CSV:', OUT_CSV)\n",
        "print('Total usable clips:', len(clean_df))\n",
        "clean_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eaf84b8",
      "metadata": {
        "id": "3eaf84b8"
      },
      "source": [
        "\n",
        "### Prepare dataset for training\n",
        "\n",
        "The `finetune-hf-vits` script can read from a HuggingFace dataset or from local CSV.  \n",
        "This step creates a cleaned CSV file that can be directly used by the training script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5085d64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "5bbe1dd822ef4f1e9b18e022d6004ac9",
            "ecaae89771af441bba58244a7ddfcb9d",
            "0b464c65ddb84c54bd42f164ccb0d0f4",
            "15df518a41d14af695a82859a6bc75d2",
            "f3a4f6e4a41d44a5abb98977f7b5afdd",
            "467a7fbcae96452c885391eb4fea0bbc",
            "c1496daec1ea4938958681b8d9c8b704",
            "84f8e4f99d6246f39f1c4722e8f83ea5",
            "37b964789a4e4cfe8f597c657cbe6654",
            "fcf593aa563b469ba537afb4f0a7754c",
            "7b03ee6107e84dda971f18493df63178"
          ]
        },
        "id": "b5085d64",
        "outputId": "78baf024-ba40-486b-eef9-1a0cf06467c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/521 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bbe1dd822ef4f1e9b18e022d6004ac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved HuggingFace dataset at: /content/drive/MyDrive/thai_tts/work/hf_dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV and create a cleaned version for training\n",
        "df = pd.read_csv(OUT_CSV)\n",
        "\n",
        "# Ensure column names match training script expectations\n",
        "# The training script expects 'path' and 'text' columns\n",
        "if 'file_name' in df.columns:\n",
        "    df = df.rename(columns={'file_name': 'path'})\n",
        "\n",
        "# Save the cleaned CSV that can be directly used by the training script\n",
        "cleaned_csv_path = WORK_DIR / 'metadata_clean.csv'\n",
        "df.to_csv(cleaned_csv_path, index=False)\n",
        "print(f'Saved cleaned CSV for training at: {cleaned_csv_path}')\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'Columns: {list(df.columns)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbeddc2",
      "metadata": {
        "id": "adbeddc2"
      },
      "source": [
        "\n",
        "## Prepare MMS-TTS (Thai) checkpoint for training\n",
        "\n",
        "This uses `convert_original_discriminator_checkpoint.py` from the recipe to create a trainable VITS-style checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db90c60a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db90c60a",
        "outputId": "391225f6-c461-4604-8868-5143848607e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits\n",
            "2025-09-19 05:38:01.243442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758260281.263555    2603 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758260281.269682    2603 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758260281.285422    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285450    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285454    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260281.285467    2603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-19 05:38:01.290114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "full_models/tha/D_100000.pth: 100% 561M/561M [00:07<00:00, 74.4MB/s]\n",
            "config.json: 1.64kB [00:00, 8.49MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/config.json\n",
            "Model config VitsConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"architectures\": [\n",
            "    \"VitsModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"depth_separable_channels\": 2,\n",
            "  \"depth_separable_num_layers\": 3,\n",
            "  \"discriminator_kernel_size\": 5,\n",
            "  \"discriminator_period_channels\": [\n",
            "    1,\n",
            "    32,\n",
            "    128,\n",
            "    512,\n",
            "    1024\n",
            "  ],\n",
            "  \"discriminator_periods\": [\n",
            "    2,\n",
            "    3,\n",
            "    5,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"discriminator_scale_channels\": [\n",
            "    1,\n",
            "    16,\n",
            "    64,\n",
            "    256,\n",
            "    1024\n",
            "  ],\n",
            "  \"discriminator_stride\": 3,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"duration_predictor_dropout\": 0.5,\n",
            "  \"duration_predictor_filter_channels\": 256,\n",
            "  \"duration_predictor_flow_bins\": 10,\n",
            "  \"duration_predictor_kernel_size\": 3,\n",
            "  \"duration_predictor_num_flows\": 4,\n",
            "  \"duration_predictor_tail_bound\": 5.0,\n",
            "  \"ffn_dim\": 768,\n",
            "  \"ffn_kernel_size\": 3,\n",
            "  \"flow_size\": 192,\n",
            "  \"hidden_act\": \"relu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 192,\n",
            "  \"hop_length\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"leaky_relu_slope\": 0.1,\n",
            "  \"model_type\": \"vits\",\n",
            "  \"noise_scale\": 0.667,\n",
            "  \"noise_scale_duration\": 0.8,\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"num_speakers\": 1,\n",
            "  \"posterior_encoder_num_wavenet_layers\": 16,\n",
            "  \"prior_encoder_num_flows\": 4,\n",
            "  \"prior_encoder_num_wavenet_layers\": 4,\n",
            "  \"resblock_dilation_sizes\": [\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ]\n",
            "  ],\n",
            "  \"resblock_kernel_sizes\": [\n",
            "    3,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"sampling_rate\": 16000,\n",
            "  \"segment_size\": 8192,\n",
            "  \"speaker_embedding_size\": 0,\n",
            "  \"speaking_rate\": 1.0,\n",
            "  \"spectrogram_bins\": 513,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"upsample_initial_channel\": 512,\n",
            "  \"upsample_kernel_sizes\": [\n",
            "    16,\n",
            "    16,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"upsample_rates\": [\n",
            "    8,\n",
            "    8,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"use_bias\": true,\n",
            "  \"use_stochastic_duration_prediction\": true,\n",
            "  \"vocab_size\": 71,\n",
            "  \"wavenet_dilation_rate\": 1,\n",
            "  \"wavenet_dropout\": 0.0,\n",
            "  \"wavenet_kernel_size\": 5,\n",
            "  \"window_size\": 4\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/config.json\n",
            "Model config VitsConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"architectures\": [\n",
            "    \"VitsModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"depth_separable_channels\": 2,\n",
            "  \"depth_separable_num_layers\": 3,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"duration_predictor_dropout\": 0.5,\n",
            "  \"duration_predictor_filter_channels\": 256,\n",
            "  \"duration_predictor_flow_bins\": 10,\n",
            "  \"duration_predictor_kernel_size\": 3,\n",
            "  \"duration_predictor_num_flows\": 4,\n",
            "  \"duration_predictor_tail_bound\": 5.0,\n",
            "  \"ffn_dim\": 768,\n",
            "  \"ffn_kernel_size\": 3,\n",
            "  \"flow_size\": 192,\n",
            "  \"hidden_act\": \"relu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 192,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"leaky_relu_slope\": 0.1,\n",
            "  \"model_type\": \"vits\",\n",
            "  \"noise_scale\": 0.667,\n",
            "  \"noise_scale_duration\": 0.8,\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"num_speakers\": 1,\n",
            "  \"posterior_encoder_num_wavenet_layers\": 16,\n",
            "  \"prior_encoder_num_flows\": 4,\n",
            "  \"prior_encoder_num_wavenet_layers\": 4,\n",
            "  \"resblock_dilation_sizes\": [\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      1,\n",
            "      3,\n",
            "      5\n",
            "    ]\n",
            "  ],\n",
            "  \"resblock_kernel_sizes\": [\n",
            "    3,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"sampling_rate\": 16000,\n",
            "  \"speaker_embedding_size\": 0,\n",
            "  \"speaking_rate\": 1.0,\n",
            "  \"spectrogram_bins\": 513,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"upsample_initial_channel\": 512,\n",
            "  \"upsample_kernel_sizes\": [\n",
            "    16,\n",
            "    16,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"upsample_rates\": [\n",
            "    8,\n",
            "    8,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"use_bias\": true,\n",
            "  \"use_stochastic_duration_prediction\": true,\n",
            "  \"vocab_size\": 71,\n",
            "  \"wavenet_dilation_rate\": 1,\n",
            "  \"wavenet_dropout\": 0.0,\n",
            "  \"wavenet_kernel_size\": 5,\n",
            "  \"window_size\": 4\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 145M/145M [00:01<00:00, 96.6MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/model.safetensors\n",
            "All model checkpoint weights were used when initializing VitsModel.\n",
            "\n",
            "All the weights of VitsModel were initialized from the model checkpoint at facebook/mms-tts-tha.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VitsModel for predictions without further training.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "model loaded: 46.7M params\n",
            "tokenizer_config.json: 100% 289/289 [00:00<00:00, 1.98MB/s]\n",
            "vocab.json: 100% 902/902 [00:00<00:00, 6.77MB/s]\n",
            "special_tokens_map.json: 100% 49.0/49.0 [00:00<00:00, 336kB/s]\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/vocab.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mms-tts-tha/snapshots/fcecc37a91566f4a36ba6c6c8aa39830ff6daa9d/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file chat_template.jinja from cache at None\n",
            "Configuration saved in /content/mms-tha-train/config.json\n",
            "Model weights saved in /content/mms-tha-train/model.safetensors\n",
            "tokenizer config file saved in /content/mms-tha-train/tokenizer_config.json\n",
            "Special tokens file saved in /content/mms-tha-train/special_tokens_map.json\n",
            "added tokens file saved in /content/mms-tha-train/added_tokens.json\n",
            "Feature extractor saved in /content/mms-tha-train/preprocessor_config.json\n",
            "total 317M\n",
            "drwxr-xr-x 2 root root 4.0K Sep 19 05:38 .\n",
            "drwxr-xr-x 1 root root 4.0K Sep 19 05:38 ..\n",
            "-rw-r--r-- 1 root root   18 Sep 19 05:38 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.0K Sep 19 05:38 config.json\n",
            "-rw-r--r-- 1 root root 317M Sep 19 05:38 model.safetensors\n",
            "-rw-r--r-- 1 root root  254 Sep 19 05:38 preprocessor_config.json\n",
            "-rw-r--r-- 1 root root   49 Sep 19 05:38 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  704 Sep 19 05:38 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  902 Sep 19 05:38 vocab.json\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/finetune-hf-vits\n",
        "\n",
        "!python convert_original_discriminator_checkpoint.py   --language_code {LANG_CODE}   --pytorch_dump_folder_path /content/mms-tha-train\n",
        "\n",
        "# (Optional) list files\n",
        "!ls -lah /content/mms-tha-train\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c82a17",
      "metadata": {
        "id": "74c82a17"
      },
      "source": [
        "\n",
        "## Training config\n",
        "\n",
        "Adjust batch size, learning rate, max steps/epochs to your GPU and dataset size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0f0ccca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0f0ccca",
        "outputId": "06cca9b9-0bbc-468c-d4e4-4548ee235f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_name_or_path\": \"/content/mms-tha-train\",\n",
            "  \"dataset_name\": \"/content/drive/MyDrive/thai_tts/work/hf_dataset\",\n",
            "  \"audio_column_name\": \"path\",\n",
            "  \"text_column_name\": \"text\",\n",
            "  \"train_split_name\": \"train\",\n",
            "  \"eval_split_name\": \"train\",\n",
            "  \"output_dir\": \"/content/tts-tha-checkpoints\",\n",
            "  \"do_train\": true,\n",
            "  \"per_device_train_batch_size\": 8,\n",
            "  \"gradient_accumulation_steps\": 2,\n",
            "  \"learning_rate\": 0.0001,\n",
            "  \"max_steps\": 20000,\n",
            "  \"logging_steps\": 50,\n",
            "  \"save_steps\": 1000,\n",
            "  \"eval_steps\": 1000,\n",
            "  \"warmup_steps\": 500,\n",
            "  \"fp16\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json, os\n",
        "cfg = {\n",
        "  # Model arguments\n",
        "  \"model_name_or_path\": \"/content/mms-tha-train\",\n",
        "  \n",
        "  # Data arguments - using CSV instead of save_to_disk format\n",
        "  \"dataset_name\": str(WORK_DIR / 'metadata_clean.csv'),\n",
        "  \"audio_column_name\": \"path\",\n",
        "  \"text_column_name\": \"text\",\n",
        "  \"train_split_name\": \"train\",\n",
        "  \"eval_split_name\": \"train\",\n",
        "  \n",
        "  # Training arguments\n",
        "  \"output_dir\": \"/content/tts-tha-checkpoints\",\n",
        "  \"do_train\": True,\n",
        "  \"per_device_train_batch_size\": 8,\n",
        "  \"gradient_accumulation_steps\": 2,\n",
        "  \"learning_rate\": 0.0001,\n",
        "  \"max_steps\": 20000,\n",
        "  \"logging_steps\": 50,\n",
        "  \"save_steps\": 1000,\n",
        "  \"eval_steps\": 1000,\n",
        "  \"warmup_steps\": 500,\n",
        "  \"fp16\": True\n",
        "}\n",
        "os.makedirs('/content/config', exist_ok=True)\n",
        "with open('/content/config/train_thai.json', 'w') as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "print(open('/content/config/train_thai.json').read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b810967e",
      "metadata": {
        "id": "b810967e"
      },
      "source": [
        "\n",
        "## Launch training\n",
        "\n",
        "This will start fine-tuning the Thai MMS-TTS model on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "EIOqSH8Tfzvf",
        "outputId": "c8d30cde-7283-49d4-fd41-4a0e8ac758f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "EIOqSH8Tfzvf",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/finetune-hf-vits/monotonic_align"
      ],
      "metadata": {
        "id": "1idEwjr9f995",
        "outputId": "b77b0e62-2dc0-4e95-e42a-deeb558498d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1idEwjr9f995",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits/monotonic_align\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "6MpWFW16gAyB"
      },
      "id": "6MpWFW16gAyB",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dbf7b6c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbf7b6c4",
        "outputId": "44eb9521-776d-406a-96e3-d208f7c6ba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetune-hf-vits\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-09-19 05:45:06.625833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758260706.661256    4490 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758260706.671421    4490 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758260706.698965    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699009    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699017    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758260706.699022    4490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-19 05:45:06.706729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetune-hf-vits/run_vits_finetuning.py\", line 1494, in <module>\n",
            "    main()\n",
            "  File \"/content/finetune-hf-vits/run_vits_finetuning.py\", line 534, in main\n",
            "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/hf_argparser.py\", line 420, in parse_json_file\n",
            "    outputs = self.parse_dict(data, allow_extra_keys=allow_extra_keys)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/hf_argparser.py\", line 396, in parse_dict\n",
            "    raise ValueError(f\"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}\")\n",
            "ValueError: Some keys are not used by the HfArgumentParser: ['dataset', 'train']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1235, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 823, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'run_vits_finetuning.py', '/content/config/train_thai.json']' returned non-zero exit status 1.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/finetune-hf-vits\n",
        "!accelerate launch run_vits_finetuning.py /content/config/train_thai.json\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697e8777",
      "metadata": {
        "id": "697e8777"
      },
      "source": [
        "\n",
        "## Quick inference test\n",
        "\n",
        "Generate a sample audio using the fine-tuned checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5006385",
      "metadata": {
        "id": "f5006385"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "import soundfile as sf\n",
        "\n",
        "ckpt_dir = \"/content/tts-tha-checkpoints\"\n",
        "tts = pipeline(\"text-to-speech\", model=ckpt_dir, device=0 if torch.cuda.is_available() else -1)\n",
        "sample_text = \"สวัสดีครับ ยินดีที่ได้รู้จัก นี่คือระบบสังเคราะห์เสียงภาษาไทยที่ฝึกด้วยข้อมูลของเรา\"\n",
        "out = tts(sample_text)\n",
        "sf.write('/content/sample_thai_tts.wav', out[\"audio\"], out[\"sampling_rate\"], subtype='PCM_16')\n",
        "print('Saved:', '/content/sample_thai_tts.wav')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b2fe3a",
      "metadata": {
        "id": "06b2fe3a"
      },
      "source": [
        "\n",
        "## Save to Drive (and optionally push to Hugging Face Hub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25ec607",
      "metadata": {
        "id": "f25ec607"
      },
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "drive_ckpt = str(WORK_DIR / 'checkpoints_mms_thai')\n",
        "shutil.copytree('/content/tts-tha-checkpoints', drive_ckpt, dirs_exist_ok=True)\n",
        "print('Copied checkpoints to:', drive_ckpt)\n",
        "\n",
        "# Optional: push to Hub (uncomment and set your repo)\n",
        "# !pip -q install huggingface_hub\n",
        "# from huggingface_hub import HfApi, create_repo\n",
        "# repo_id = \"yourname/mms-thai-finetuned\"\n",
        "# create_repo(repo_id, private=True, exist_ok=True)\n",
        "# !huggingface-cli login\n",
        "# !git lfs install\n",
        "# !git init /content/tts-tha-checkpoints\n",
        "# %cd /content/tts-tha-checkpoints\n",
        "# !git remote add origin https://huggingface.co/yourname/mms-thai-finetuned\n",
        "# !git add . && git commit -m \"Add Thai TTS fine-tuned\" && git push -u origin main\n",
        "# %cd /content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e07531b",
      "metadata": {
        "id": "6e07531b"
      },
      "source": [
        "\n",
        "### Notes & Tips\n",
        "- Recommended 1–3 hours clean audio for decent cloning; 5–10+ hours for robust style control.\n",
        "- Keep clips 1–12 seconds; remove background noise as much as possible.\n",
        "- If you get OOM (out-of-memory), reduce `per_device_train_batch_size` or increase `gradient_accumulation_steps`.\n",
        "- If training is slow, reduce `max_steps` initially to validate the pipeline, then scale up.\n",
        "\n",
        "### 🎯 Current Approach: CSV-based Audio Loading\n",
        "This notebook now uses CSV-based audio loading by default, which is more robust:\n",
        "1. ✅ Audio files are loaded directly from disk (not stored in HuggingFace datasets)\n",
        "2. ✅ Better memory management and compatibility\n",
        "3. ✅ Automatic audio format conversion and resampling\n",
        "4. ✅ The training script automatically detects `.csv` files and uses the new loading method\n",
        "\n",
        "**Your CSV should have these columns:**\n",
        "- `path`: Full path to the audio file\n",
        "- `text`: Corresponding text transcription\n",
        "\n",
        "**Example CSV:**\n",
        "```\n",
        "path,text\n",
        "/path/to/audio1.wav,สวัสดีครับ\n",
        "/path/to/audio2.wav,ขอบคุณมากครับ\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5bbe1dd822ef4f1e9b18e022d6004ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecaae89771af441bba58244a7ddfcb9d",
              "IPY_MODEL_0b464c65ddb84c54bd42f164ccb0d0f4",
              "IPY_MODEL_15df518a41d14af695a82859a6bc75d2"
            ],
            "layout": "IPY_MODEL_f3a4f6e4a41d44a5abb98977f7b5afdd"
          }
        },
        "ecaae89771af441bba58244a7ddfcb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467a7fbcae96452c885391eb4fea0bbc",
            "placeholder": "​",
            "style": "IPY_MODEL_c1496daec1ea4938958681b8d9c8b704",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "0b464c65ddb84c54bd42f164ccb0d0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f8e4f99d6246f39f1c4722e8f83ea5",
            "max": 521,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37b964789a4e4cfe8f597c657cbe6654",
            "value": 521
          }
        },
        "15df518a41d14af695a82859a6bc75d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf593aa563b469ba537afb4f0a7754c",
            "placeholder": "​",
            "style": "IPY_MODEL_7b03ee6107e84dda971f18493df63178",
            "value": " 521/521 [00:01&lt;00:00, 381.93 examples/s]"
          }
        },
        "f3a4f6e4a41d44a5abb98977f7b5afdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467a7fbcae96452c885391eb4fea0bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1496daec1ea4938958681b8d9c8b704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84f8e4f99d6246f39f1c4722e8f83ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b964789a4e4cfe8f597c657cbe6654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcf593aa563b469ba537afb4f0a7754c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b03ee6107e84dda971f18493df63178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}